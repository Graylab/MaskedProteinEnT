{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MaskedProteinEnT: Sample Training Notebook\n",
    "\n",
    "This notebook demonstrates how to train a **MaskedProteinEnT** using the PyTorch framework. The model leverages masked language modeling for protein sequences.\n",
    "\n",
    "**Repository**: [MaskedProteinEnT](https://github.com/Graylab/MaskedProteinEnT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation\n",
    "- Requires access to a GPU with **CUDA 11.1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/Graylab/MaskedProteinEnT.git\n",
    "!pip install torch==1.9.1+cu111 torchvision==0.10.1+cu111 torchaudio==0.9.1 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "!pip install -r requirements_torch191.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading training datasets\n",
    "- Run this cell to download the datasets required for either training or fine-tuning\n",
    "- Datasets to the required urls for initial training are already uncommented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the directory if it doesn't exist\n",
    "! mkdir -p training_datasets\n",
    "\n",
    "urls_content = \"\"\"\\\n",
    "## Uncomment the URLs you want to download and comment out those you want to skip\n",
    "## Required for initial training\n",
    "https://zenodo.org/record/13831403/files/ids_train_casp12nr50_nr70Ig_nr40Others.fasta\n",
    "https://zenodo.org/record/13831403/files/sidechainnet_casp12_50.pkl\n",
    "## Fine-tuning datasets\n",
    "# https://zenodo.org/record/13831403/files/AbSCSAbDAb_trainnr90_bkandcbcoords_aug2022.h5\n",
    "# https://zenodo.org/record/13831403/files/ppi_trainset_5032_noabag_aug2022.h5\n",
    "# https://zenodo.org/record/13831403/files/train_af_paired_nr70.h5\n",
    "\"\"\"\n",
    "\n",
    "# Write the content to urls.txt\n",
    "with open('urls.txt', 'w') as f:\n",
    "    f.write(urls_content)\n",
    "\n",
    "# Download files in parallel using wget and xargs, ignoring commented lines\n",
    "! grep -v '^#' urls.txt | xargs -n 1 -P 5 wget -nc -P training_datasets\n",
    "\n",
    "# Clean up\n",
    "! rm urls.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjust model and training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYERS=4\n",
    "HEADS=8\n",
    "DIM=256\n",
    "OLD=0\n",
    "BS=1\n",
    "SS=50 # protein model was trained on 90ss\n",
    "save_every=5\n",
    "gmodel='egnn-trans-ma'\n",
    "atom_types='backbone_and_cb'\n",
    "NN=48\n",
    "EPOCHS=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set training seed, output directory and wandb entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED=1\n",
    "MODELS_DIR=f'models_out_dir_seed_{SEED}'\n",
    "WANDB_ENTITY=\"YOUR_WANDB_ENTITY\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLURM Job and GPU Setup\n",
    "\n",
    "The following code retrieves environment variables set by SLURM, a workload manager commonly used for scheduling jobs on HPC systems. It performs the following steps:\n",
    "\n",
    "- **SLURM Environment Variables**: Retrieves the total number of tasks (`SLURM_NTASKS`) and the number of nodes (`SLURM_JOB_NUM_NODES`) for the current job, using default values of 6 and 1 if these are not set.\n",
    "- **Process Calculation**: Calculates the number of processes per node (`n_procs`) by dividing the total number of tasks by the number of nodes.\n",
    "- **GPU Information**: Retrieves the list of GPUs assigned to the job (`SLURM_STEP_GPUS`) and counts the number of GPUs being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of tasks (SLURM_NTASKS) or default to 6\n",
    "slurm_ntasks = int(os.getenv('SLURM_NTASKS', 6))  # Default to 6 if not set\n",
    "\n",
    "# Get the number of nodes (SLURM_JOB_NUM_NODES) or default to 1\n",
    "slurm_job_num_nodes = int(os.getenv('SLURM_JOB_NUM_NODES', 1))  # Default to 1 if not set\n",
    "\n",
    "# Calculate n_procs (number of processes per node)\n",
    "n_procs = slurm_ntasks // slurm_job_num_nodes\n",
    "\n",
    "# Default value for num_gpus\n",
    "num_gpus = 1\n",
    "\n",
    "# Get the list of GPUs (SLURM_STEP_GPUS) or default to a single GPU\n",
    "slurm_step_gpus = os.getenv('SLURM_STEP_GPUS', \"0\")  # Default to GPU 0 if not set\n",
    "gpus_list = slurm_step_gpus.split(',')\n",
    "num_gpus = len(gpus_list)\n",
    "\n",
    "print(f\"n_procs = {n_procs}\")\n",
    "print(f\"num_gpus = {num_gpus}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final preparation before running training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COPY SIDECHAINNET TO LOCAL TEMPORARY DIRECTORY\n",
    "sidechainnet = './training_datasets/sidechainnet_casp12_50.pkl'\n",
    "sidechainnet_temp = '/tmp/sidechainnet_casp12_50.pkl'\n",
    "\n",
    "if not os.path.exists(sidechainnet_temp):\n",
    "    shutil.copy(sidechainnet, sidechainnet_temp)\n",
    "\n",
    "gd2_dataset_ids = os.path.join(os.getcwd(), 'training_datasets', 'ids_train_casp12nr50_nr70Ig_nr40Others.fasta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training using shell execution in Jupyter\n",
    "# !python3 ./MaskedProteinEnT/train_masked_model.py \\ ### Uncomment when final\n",
    "!python3 ./train_masked_model.py \\\n",
    "  --save_every {save_every} --lr 0.00001 --batch_size {BS} \\\n",
    "  --heads {HEADS} --model_dim {DIM} --epochs {EPOCHS} --dropout 0.2 \\\n",
    "  --masking_rate_max 0.25 --topk_metrics 1 --layers {LAYERS} \\\n",
    "  --num_gpus {num_gpus} --crop_sequences \\\n",
    "  --scn_sequence_similarity {SS} --protein_gmodel {gmodel} \\\n",
    "  --lr_patience 350 --lr_cooldown 20 --max_ag_neighbors {NN} \\\n",
    "  --atom_types {atom_types} \\\n",
    "  --file_with_selected_scn_ids_for_training {gd2_dataset_ids} \\\n",
    "  --lightning_save_last_model --use_scn --num_procs {n_procs} \\\n",
    "  --output_dir {MODELS_DIR} --seed {SEED} --wandb_entity {WANDB_ENTITY}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
